<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Justin Nguyen / Software Engineer</title><link>https://www.justinnuwin.com/</link><description>Recent content on Justin Nguyen / Software Engineer</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© 2020 Justin Nguyen</copyright><lastBuildDate>Tue, 22 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://www.justinnuwin.com/index.xml" rel="self" type="application/rss+xml"/><item><title>IRIS Flight Software Development</title><link>https://www.justinnuwin.com/projects/iris-rover/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/projects/iris-rover/</guid><description>Github
I am currently developing the C&amp;amp;DH flight software for IRIS, CMU&amp;rsquo;s Lunar Rover. The flight software uses NASA JPL&amp;rsquo;s F-Prime flight-software framework on top of FreeRTOS. The C&amp;amp;DH is a TI Hercules Safety-Microcontroller.
Links: Github</description><content>&lt;p>&lt;a href="https://github.com/PlanetaryRobotics/CubeRoverPackage">Github&lt;/a>&lt;/p>
&lt;p>I am currently developing the C&amp;amp;DH flight software for IRIS, CMU&amp;rsquo;s Lunar Rover.
The flight software uses NASA JPL&amp;rsquo;s F-Prime flight-software framework on top of FreeRTOS.
The C&amp;amp;DH is a TI Hercules Safety-Microcontroller.&lt;/p>
&lt;h4 id="links">Links:&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/PlanetaryRobotics/CubeRoverPackage">Github&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Lawrence Livermore National Lab</title><link>https://www.justinnuwin.com/experience/llnl-2020/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/experience/llnl-2020/</guid><description>I originally interned at LLNL over the summer of 2017 (click hear to learn more about my group), and joined part-time during university to support the software side of the project longer-term. To facilitate me working at the national lab part-time during university I am contracted under J&amp;amp;A Film Preservation, a company that has been working with the Film Scanning and Re-Analysis Project for several years. I am continuing to develop vision algorithms for scientific measurements, but I am also working on streamlining our team&amp;rsquo;s workflow by using the latest workflow automation tools like Dagster.</description><content>&lt;p>&lt;a href="https://www.justinnuwin.com/experience/llnl-2017">I originally interned at LLNL over the summer of 2017 (click hear to learn more about my group)&lt;/a>, and joined part-time during university to support the software side of the project longer-term.
To facilitate me working at the national lab part-time during university I am contracted under J&amp;amp;A Film Preservation, a company that has been working with the Film Scanning and Re-Analysis Project for several years.
I am continuing to develop vision algorithms for scientific measurements, but I am also working on streamlining our team&amp;rsquo;s workflow by using the latest workflow automation tools like Dagster.&lt;/p>
&lt;p>Over the past 2 years working for the Film Scanning and Re-analysis project I have added several new measurement tools to our measurement tool written in Python.
These tools include &lt;strong>global-segmented&lt;/strong> film exposure measurement methods, and a a new fireball detection pipeline that I architected utilizing spectral analysis and temporal edge tracking.
I also added measurement statistics for all of the data our group generates using scikit-learn to better understand our measurements.
In addition, I have refactored our codebase to explicitly support both CLI and GUI (Tk) methods to run which will allow the team to programmatically re-analyze and update old results more easily.&lt;/p>
&lt;p>From the refactoring sprint, I was able to abstract our application logic to work well on both desktop environments for developing new codes and on HPC environments provided by Livermore Computing&amp;rsquo;s supercomputers.
Dagster, a workflow orchestration tool, ties our workflow together with its pipeline and asset management systems, and I developed a multithreaded image loader as a CPython module to further speed up our program&amp;rsquo;s analysis.&lt;/p></content></item><item><title>Evaluation of ASR in Musical Environments</title><link>https://www.justinnuwin.com/projects/asr-music-semester-project/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/projects/asr-music-semester-project/</guid><description>Github
ASR is used heavily in eyes-busy or hands-busy situations and often time the user may be speaking over noise. My peers and I are particularly interested in how music effects ASR decoding. We use several music datasets of varied genre or broken-down instrumentation to allow us to perform in-depth anaysis of how different aspects of music influences speech recognizer&amp;rsquo;s performance. We then train a new model from what we have learned to see if we could improve the original model&amp;rsquo;s performance.</description><content>&lt;p>&lt;a href="https://github.com/justinnuwin/Evaluation-of-ASR-in-Musical-Environment">Github&lt;/a>&lt;/p>
&lt;p>ASR is used heavily in eyes-busy or hands-busy situations and often time the user may be speaking over noise.
My peers and I are particularly interested in how music effects ASR decoding.
We use several music datasets of varied genre or broken-down instrumentation to allow us to perform in-depth anaysis of how different aspects of music influences speech recognizer&amp;rsquo;s performance.
We then train a new model from what we have learned to see if we could improve the original model&amp;rsquo;s performance.&lt;/p>
&lt;img src="https://www.justinnuwin.com/projects/asr-music-semester-project/poster.png" alt="ASR Music Evaluation semester project poster" class="center" />
&lt;h4 id="links">Links:&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/justinnuwin/Evaluation-of-ASR-in-Musical-Environment/blob/master/18-781_Project_Report.pdf">Paper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/projects/asr-music-semester-project/poster.pdf">Poster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/justinnuwin/Evaluation-of-ASR-in-Musical-Environment">Github&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/justinnuwin/Evaluation-of-ASR-in-Musical-Environment#releases">Data&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Embedded LS-PIV for Measuring Stream Flows</title><link>https://www.justinnuwin.com/projects/lspiv-semester-project/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/projects/lspiv-semester-project/</guid><description>Bitbucket not currently public
The USGS measures stream flows since they are critical for long-term tracking and modeling/forecasting to ensure that federal water priorities and responsibilities can be met and that the nation’s rivers canbe effectively managed. Recently, the USGS has been looking at non-contact methods of collection this data which would allow USGS scientists to gather data more safely and possibly without even going in to the field. One such method is large-scale particle image velocimetry (LS-PIV).</description><content>&lt;p>&lt;a href="#">Bitbucket &lt;strong>not currently public&lt;/strong>&lt;/a>&lt;/p>
&lt;p>The USGS measures stream flows since they are critical for long-term tracking and modeling/forecasting to ensure that federal water priorities and responsibilities can be met and that the nation’s rivers canbe effectively managed.
Recently, the USGS has been looking at non-contact methods of collection this data which would allow USGS scientists to gather data more safely and possibly without even going in to the field.
One such method is large-scale particle image velocimetry (LS-PIV).
LS-PIV is a special case of algorithms that perform optical flow; at its is a cross correlation operation which is expensive in both time complexity and energy usage.
My peers and I optimize the PIV algorithm to run on a 32-bit embedded microcontroller that can quickly and accurately perform the PIV computation and have a battery life of up to 2 years.&lt;/p>
&lt;img src="https://www.justinnuwin.com/projects/lspiv-semester-project/poster.png" alt="Embedded LS-PIV semester project poster" class="center" />
&lt;h4 id="links">Links:&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://www.justinnuwin.com/projects/lspiv-semester-project/paper.pdf">Paper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/projects/lspiv-semester-project/presentation.pdf">Presentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/projects/lspiv-semester-project/poster.pdf">Poster&lt;/a>&lt;/li>
&lt;li>&lt;a href="#">Bitbucket &lt;strong>not currently public&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Denoising EEG Signals</title><link>https://www.justinnuwin.com/projects/denoising-eeg-semester-project/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/projects/denoising-eeg-semester-project/</guid><description>Github
EEGs are extremely interesting to the signal processing world as the signals from them are high dimensional and localized spatially, spectrally, and temporally. These signals are extremely low amplitude and artifacts appear in the measured signal due to normal human processes like blinking, breathing, and moving; or noise can appear due to the external factors such as line noise or sound in the room. My peers and I implement Viola&amp;rsquo;s ICA CorrMap procedure to denoise EEGs, but rather than retreiving EEG artifacts in-situ, we use Cho&amp;rsquo;s EEG dataset which separately records subjects in both rest states and performing various &amp;lsquo;noisy&amp;rsquo; actions prior to a motor imagery trial.</description><content>&lt;p>&lt;a href="https://github.com/justinnuwin/Denoising-EEG-Signals">Github&lt;/a>&lt;/p>
&lt;p>EEGs are extremely interesting to the signal processing world as the signals from them are high dimensional and localized spatially, spectrally, and temporally.
These signals are extremely low amplitude and artifacts appear in the measured signal due to normal human processes like blinking, breathing, and moving; or noise can appear due to the external factors such as line noise or sound in the room.
My peers and I implement Viola&amp;rsquo;s ICA CorrMap procedure to denoise EEGs, but rather than retreiving EEG artifacts in-situ, we use Cho&amp;rsquo;s EEG dataset which separately records subjects in both rest states and performing various &amp;lsquo;noisy&amp;rsquo; actions prior to a motor imagery trial.
We use this data to try and denoise signals and try to devise a way to create a generalized artifact template that can be transferred from user to user.&lt;/p>
&lt;h4 id="links">Links:&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/justinnuwin/Denoising-EEG-Signals/blob/master/18-797_Project_Report.pdf">Paper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/justinnuwin/Denoising-EEG-Signals">Github&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Cal Poly CubeSat Lab (PolySat)</title><link>https://www.justinnuwin.com/experience/polysat-2020/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/experience/polysat-2020/</guid><description>I was heavily involved in the Cal Poly CubeSat Lab (PolySat) during my undergraduate. While working there for 4 years, I have worked on many CubeSat missions in a variety of roles throughout various mission lifetimes. I list out my accomplishmets and various projects below. I also interned at JPL, working on the MarCO CubeSat which was highly relavent to my experience at the Cal Poly CubeSat Lab.
Electrical Engineer 2016 - 2020</description><content>&lt;p>I was heavily involved in the &lt;a href="https://www.polysat.org">Cal Poly CubeSat Lab (PolySat)&lt;/a> during my undergraduate.
While working there for 4 years, I have worked on many CubeSat missions in a variety of roles throughout various mission lifetimes.
I list out my accomplishmets and various projects below.
I also interned at &lt;a href="https://www.justinnuwin.com/experience/jpl-2018">JPL, working on the MarCO CubeSat&lt;/a> which was highly relavent to my experience at the Cal Poly CubeSat Lab.&lt;/p>
&lt;h3 id="electrical-engineer">Electrical Engineer&lt;/h3>
&lt;p>&lt;strong>2016 - 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Rigid and Flexible PCB design in OrCAD and Altium&lt;/li>
&lt;li>Lead electrical design reviews&lt;/li>
&lt;li>Design and test side panels and custom flat flex cables for 3 CubeSat missions (CP12-ExoCube2, CP14-ADE, C-SPOT)&lt;/li>
&lt;li>Assist in design and testing of payload interface boards and various peripheral electronics for 6 CubeSat missions (CP10-ISX, CP12-ExoCube2, CP14-ADE, CP15-Spinnacker3, Falcon, C-SPOT)&lt;/li>
&lt;li>Design updated ground support debugger for CubeSat flight computer utilizing USB&lt;/li>
&lt;li>Oversee electrical - software interface and functionality testing&lt;/li>
&lt;li>Test, verify, and validate flight electronic systems (Power, Communications, Command &amp;amp; Data Handling (C&amp;amp;DH))&lt;/li>
&lt;li>Develop and troubleshoot embedded Linux flight computer utilizing Atmel processor&lt;/li>
&lt;li>Tune and checkout flight UHF transceivers&lt;/li>
&lt;li>Tune antennas using VNA (vector network analyzer)&lt;/li>
&lt;li>Flight Assembly in Class 100,000 Clean Room&lt;/li>
&lt;li>Review and improve testing procedures&lt;/li>
&lt;li>Write documentation for assembly procedures, board modification procedures, and OrCAD design tutorials&lt;/li>
&lt;li>Train new members in electrical system design, high speed digital design, and PCB design using OrCAD and Altium&lt;/li>
&lt;/ul>
&lt;h3 id="groundstation-lead">Groundstation Lead&lt;/h3>
&lt;p>&lt;strong>2017 - 2019&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Organize operations team for various flight missions: CP-7 DAVE, CP-10 ISX, CP-12 ExoCube II&lt;/li>
&lt;li>Train new members on groundstation infrastructure and operation&lt;/li>
&lt;li>Maintain Cal Poly&amp;rsquo;s 3 UHF/VHF ground stations used for operating in-orbit spacecraft&lt;/li>
&lt;li>Coordinate ground station team for spacecraft operations&lt;/li>
&lt;li>Manage ground station projects (Ground Station In a Box, Local RF Noise Measurement Project)&lt;/li>
&lt;/ul>
&lt;h3 id="lab-manager">Lab Manager&lt;/h3>
&lt;p>&lt;strong>2018 - 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Consult for the start up of new CubeSat programs (LigerSat- Phnom Penh, Cambodia; Bronco Space-Cal Poly Pamona)&lt;/li>
&lt;li>Lead lab restructuring and redevelopment effort to overhaul training, systems engineering, V&amp;amp;V (verification and validation) checkpoints, hardware development workflows, internal receiving inspections, internal review processes, documentation processes, procedure development, and management and team structures to improve lab efficiency and success&lt;/li>
&lt;li>Manage CubeSat missions and projects in Cal Poly CubeSat Lab (CP-7 DAVE, CP-12 ExoCube 2, CP-14 ADE, CP-15 Spinnacker3, SWIS, NIAC, Pocket Rocket, Sensor Development)&lt;/li>
&lt;li>Coordinate design and production timelines to ensure on-time delivery&lt;/li>
&lt;li>Review discipline team processes to maximize quality&lt;/li>
&lt;li>Delegate tasks with project and discipline leads to ensure balanced workload throughout engineering teams&lt;/li>
&lt;li>Improve on system level and electronic subsystem design for early mission designs sharing core hardware used across Cal Poly CubeSat Lab missions&lt;/li>
&lt;li>Meet with individual teams and team members to assist with testing and assembly tasks and scheduling&lt;/li>
&lt;li>Improved lab recruiting process&lt;/li>
&lt;li>Interim CP-12 ExoCube 2 Mission Manager&lt;/li>
&lt;/ul></content></item><item><title>Cal Poly Autonomous Vehicle Security Group</title><link>https://www.justinnuwin.com/experience/cpavsec-2020/</link><pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/experience/cpavsec-2020/</guid><description>The Cal Poly Autonomous Vehicle Security Group is headed by Dr. Bruce DeBruhl who is involved in security research for autonomous vehicles, IOT devices, and all things networked. The projects I have worked on in this group have been primarily focused towards the security of platoons of autonomous vehicles. Vehicle platoons require extensive communication to stay cohesive and ensure that no collisions occur. The two paradigms that vehicles communicate are vehicle-to-vehicle (V2V) using a protocol like DSRC or vehicle-to-infrastructure (V2I) usually using cellular infrastructure like 4G-LTE and 5G.</description><content>&lt;p>The Cal Poly Autonomous Vehicle Security Group is headed by Dr. Bruce DeBruhl who is involved in security research for autonomous vehicles, IOT devices, and all things networked.
The projects I have worked on in this group have been primarily focused towards the security of platoons of autonomous vehicles.
Vehicle platoons require extensive communication to stay cohesive and ensure that no collisions occur.
The two paradigms that vehicles communicate are vehicle-to-vehicle (V2V) using a protocol like DSRC or vehicle-to-infrastructure (V2I) usually using cellular infrastructure like 4G-LTE and 5G.&lt;/p>
&lt;h3 id="scale-platoon-project">Scale Platoon Project&lt;/h3>
&lt;p>The first year I joined the group, I provided assistance on a &lt;a href="https://digitalcommons.calpoly.edu/theses/2057">master&amp;rsquo;s thesis&lt;/a> and &lt;a href="https://digitalcommons.calpoly.edu/cpesp/249">senior project&lt;/a> both utilizing the same scale platoon platform using RC cars.
On these projects I developed real-time image processing pipelines to detect the other RC cars in the platoon and follow them.
This was done in C++ using the OpenCV library.
Due to the lack of imagery for &amp;ldquo;RC cars with electronics mounted on top&amp;rdquo;, pure image processing algorithms had to be used over machine learning or data-driven methods.
Some techniques used were distinct optical tracking markers, SIFT Features, optical flow, and sensor fusion were used to detect then track the movement of vehicles.
Since this had to run in real-time CUDA accelerated implementations were used along with the Nvidia Jetson platform.
In addition to the computer vision work, I also helped design the power distribution system for these vehicles and some of the microcontroller code to communicate with sensors like LiDAR, ultrasonic sensors, and IMUs.&lt;/p>
&lt;h3 id="autonomous-vehicle-simulator">Autonomous Vehicle Simulator&lt;/h3>
&lt;p>At the end of my third-year at Cal Poly, I started working on a new project in the group which was an autonomous vehicle simulator following the same line of thought for the scale platoon project, but not focusing on simulating life-sized vehicles on simulated roads.&lt;/p>
&lt;ul>
&lt;li>Develop vision-based lane following pipeline using OpenCV to allow simulated AV to drive on roads&lt;/li>
&lt;li>Develop autonomous perception and sensor fusion module for autonomous driving&lt;/li>
&lt;li>Design self-driving control architecture of simulated vehicle in residential environment using Microsoft Airsim and Unreal Engine&lt;/li>
&lt;li>Implement occupancy grid based local navigation system to navigate autonomous vehicle on residential streets&lt;/li>
&lt;li>Implement cascaded PID controller to drive autonomous vehicle&lt;/li>
&lt;/ul></content></item><item><title>potatOS</title><link>https://www.justinnuwin.com/projects/potatos/</link><pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/projects/potatos/</guid><description>Github
I developed an x86_64 operating system and kernel from scratch. The OS has virtual memory and threading implemented.
Links: Github</description><content>&lt;p>&lt;a href="https://github.com/justinnuwin/potatos">Github&lt;/a>&lt;/p>
&lt;p>I developed an x86_64 operating system and kernel from scratch.
The OS has virtual memory and threading implemented.&lt;/p>
&lt;h4 id="links">Links:&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/justinnuwin/potatos">Github&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Autonomous Navigation System for Mapping &amp; Traversing Rugged Terrain</title><link>https://www.justinnuwin.com/projects/cp-capstone/</link><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/projects/cp-capstone/</guid><description>Github
The Northrop Grumman Collaboration Project (NGCP) is a project-club at Cal Poly that participates in engineering challenges posed by Northrop Grumman. The collaboration project is with Northrop Grumman, in addition to Cal Poly Pomona with whom the work for the engineering challenge is split with. The 2019 school year had a new mission which was a rugged-terrain rescue mission. My peers and I were tasked with with sensing and intelligence for one of their new vehicles.</description><content>&lt;p>&lt;a href="https://github.com/justinnuwin/NGCP-Capstone-UGV">Github&lt;/a>&lt;/p>
&lt;p>The Northrop Grumman Collaboration Project (NGCP) is a project-club at Cal Poly that participates in engineering challenges posed by Northrop Grumman.
The collaboration project is with Northrop Grumman, in addition to Cal Poly Pomona with whom the work for the engineering challenge is split with.
The 2019 school year had a new mission which was a rugged-terrain rescue mission.
My peers and I were tasked with with sensing and intelligence for one of their new vehicles.&lt;/p>
&lt;img src="https://www.justinnuwin.com/projects/cp-capstone/poster.png" alt="ASR Music Evaluation semester project poster" class="center" />
&lt;h4 id="links">Links:&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://www.justinnuwin.com/projects/cp-capstone/report.pdf">Report&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/projects/cp-capstone/poster.png">Poster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/justinnuwin/NGCP-Capstone-UGV">Github&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>NASA Jet Propulsion Laboratory (JPL)</title><link>https://www.justinnuwin.com/experience/jpl-2018/</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/experience/jpl-2018/</guid><description>After 1.5 years working at the Cal Poly CubeSat Lab I had the opportunity to intern at JPL to operate the MarCO CubeSats. MarCO is a pair of CubeSats that were launched with NASA&amp;rsquo;s InSight Lander with the mission to:
Relay InSight&amp;rsquo;s Entry, Descent, and Landing (EDL) data back to Earth in real-time Demonstrate the capabilities of the CubeSat form-factor in deep-space applications An interesting aspect of the MarCO mission is the level of involvement required due to systematic constraints on the mission.</description><content>&lt;p>After 1.5 years &lt;a href="https://www.justinnuwin.com/experience/polysat-2020">working at the Cal Poly CubeSat Lab&lt;/a> I had the opportunity to intern at JPL to operate the &lt;a href="https://www.jpl.nasa.gov/cubesat/missions/marco.php">MarCO CubeSats&lt;/a>.
MarCO is a pair of CubeSats that were launched with NASA&amp;rsquo;s InSight Lander with the mission to:&lt;/p>
&lt;ul>
&lt;li>Relay InSight&amp;rsquo;s Entry, Descent, and Landing (EDL) data back to Earth in real-time&lt;/li>
&lt;li>Demonstrate the capabilities of the CubeSat form-factor in deep-space applications&lt;/li>
&lt;/ul>
&lt;p>An interesting aspect of the MarCO mission is the level of involvement required due to systematic constraints on the mission.
Unlike flagship missions which may have hundreds of personnel for operations alone, CubeSats are targeted for low-cost R&amp;amp;D, development, and operations.
To accomplish this JPL brought on many partners both in the development and the operations of MarCO.
&lt;a href="https://www.polysat.org/">Cal Poly and the CubeSat Lab&lt;/a> were selected to assist JPL with MarCO operations; this partnership consisted of two Cal Poly students who would intern at JPL over the duration of the mission to handle day-to-day operations and a larger team of students working from Cal Poly&amp;rsquo;s campus who would do long-term telemetry analysis and forecasting.
As an intern on the flight operations team, I interfaced the JPL operations team with the operations team at Cal Poly, in addition to my day-to-day duties: drafting and verifying uplink products, analyzing telemetry, and recommending future spacecraft activities.&lt;/p>
&lt;figure class="right" >
&lt;img src="https://www.justinnuwin.com/experience/jpl-2018/polysat-team.jpg" alt="PolySat summer students during MarCO mission" />
&lt;figcaption class="center" >Students working at the Cal Poly CubeSat Lab over the summer tour JPL while MarCO was cruising to Mars. Credit: Michael Fernandez&lt;/figcaption>
&lt;/figure>
&lt;p>The MarCO mission used NASA AMMOS AMPCS to handle the ground infrastructure and data management, which was easily accessible from the JPL campus, but it was not feasible to expose critical mission infrastructure externally, so I developed a lightweight data management system that could be more easily shared with Cal Poly.
The workflow I developed consisted of three parts:&lt;/p>
&lt;ul>
&lt;li>A HDF5 (and sqlite) database with an ingestion engine for pulling telemetry from JPL servers&lt;/li>
&lt;li>NASA OpenMCT which handled all data visualization&lt;/li>
&lt;li>A python client which interface with the telemetry database to query or update data with strict 2-way data binding protocols to ensure the sources of truth were maintained&lt;/li>
&lt;/ul>
&lt;p>In addition to the workflow for allowing external collaborators to access mission data, on the day-to-day side, many tools needed to be developed to streamline and automate more of the mission operations as the team matured and understood the idiosyncrasies of the spacecraft better.
Some of the tools I developed were:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Telemetry Quicklook&lt;/strong>: A service which aggregates spacecraft data and generates &amp;ldquo;quick-look&amp;rdquo; items in OpenMCT which summarizes spacecraft health.&lt;/li>
&lt;li>&lt;strong>Automatic Pass Report Generator&lt;/strong>: All spacecraft activities are planned and documented beforehand, but it proved laborious to annotate telemetry with specific events in these procedures, so this tool automatically collects telemetry generated during a procedure and generates a &amp;ldquo;pass report&amp;rdquo; (a pass is a term originally used for Earth-orbiting spacecraft when they would &amp;ldquo;pass&amp;rdquo; overhead) which was a nicely formatted pdf document which could be attached to the procedures and highlighted important telemetry during the activity. This tool was designed with Python and Pandoc.&lt;/li>
&lt;/ul>
&lt;img src="https://www.justinnuwin.com/experience/jpl-2018/marco-team-darkroom.jpg" alt="MarCO Operations Team in the SFOC &amp;#39;Dark Room&amp;#39;" class="center" />
&lt;p>If you would like to learn more about MarCO or InSight see these links:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.jpl.nasa.gov/cubesat/missions/marco.php">MarCO&amp;rsquo;s Website&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://solarsystem.nasa.gov/missions/mars-cube-one/in-depth/">In-depth details on MarCO&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.jpl.nasa.gov/news/press_kits/insight/">InSight Launch and Landing Press Kit&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>If you would like to learn more about CubeSats here are some helpful links:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.cubesat.org/">The CubeSat Organization (you can find developer documents here)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.nasa.gov/sites/default/files/atoms/files/nasa_csli_cubesat_101_508.pdf">CubeSats 101 by NASA CSLI&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.nasa.gov/mission_pages/cubesats/index.html">NASA CubeSat News&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Lawrence Livermore National Laboratory</title><link>https://www.justinnuwin.com/experience/llnl-2017/</link><pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/experience/llnl-2017/</guid><description>During my first summer at Cal Poly, I interned at the Lawrence Livermore National Laboratory (LLNL) working on the Film Scanning and Re-Analysis Project headed by nuclear physicist Gregory Spriggs. The goal of the project is to digitize all the nuclear test films before they decay and reanalyze the results, as many of the results from the 1940s through the early 1960s were plagued with guesswork from the manual measurement process.</description><content>&lt;p>During my first summer at Cal Poly, I interned at the Lawrence Livermore National Laboratory (LLNL) working on the &lt;a href="https://www.lanl.gov/discover/publications/national-security-science/2015-july/_assets/doc/NSS-jul2015-filmscanning.pdf">Film Scanning and Re-Analysis Project&lt;/a> headed by nuclear physicist Gregory Spriggs.
The goal of the project is to digitize all the nuclear test films before they decay and reanalyze the results, as many of the results from the 1940s through the early 1960s were plagued with guesswork from the manual measurement process.&lt;/p>
&lt;figure class="right" >
&lt;img src="https://www.justinnuwin.com/experience/llnl-2017/tesla28602_000037.png" alt="Frame from nuclear test film" />
&lt;figcaption class="center" >A frame from Operation Teapot (Tesla 28602). Credit LLNL Youtube Channel&lt;/figcaption>
&lt;/figure>
&lt;p>Shockwave analysis is one of the few methods used to determine the yield of a nuclear detonation.
To calculate the yield of these shots using the 1-D, spherical model developed by Taylor, an equivalent radius of the elliptical shockwave must be determined.
Armed with the digitized replicas of the original nuclear test films and the latest image processing tools, a methodology for segmenting the shockwave from the digitized film and measuring these shockwaves has been developed to quickly and accurately quantify the shape of these asymmetric shockwaves.
With a two-dimensional result from these asymmetric shockwaves, several methods are discussed to transform the final shockwave contour to an equivalent radius that can be used to determine the yield.&lt;/p>
&lt;figure class="center" >
&lt;img src="https://www.justinnuwin.com/experience/llnl-2017/exposure-adj.png" alt="Exposure adjustment results" />
&lt;figcaption class="center" >Optimized exposure LUTs derived from parameter search. First row contains the original cropped frames. The second row shows the waveforms (horizontal spectrogram) of the images in the first row. Middle row is the LUT's exposure curve. The final two rows are the adjusted frames and their respective waveforms.&lt;/figcaption>
&lt;/figure>
&lt;p>The pipeline I developed provides was developed in Python using the OpenCV library and can detect and accurately measure the shockwave and fireball with minimal user input.
I utilized the high performance computing cluster at LLNL to perform parameter search over different exposures-adjusting look up tables which could be applied to a film to &amp;ldquo;boost&amp;rdquo; the fireball&amp;rsquo;s signal.&lt;/p>
&lt;p>More detailed information about my project can be found at these links:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.justinnuwin.com/experience/llnl-2017/LLNL-TR-738324.pdf">Technical Report&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/experience/llnl-2017/LLNL-POST-735703.pdf">Poster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/experience/llnl-2017/LLNL-PRES-738070.pdf">Presentation&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>If you would like to learn more about the science behind the nuclear tests see these links:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/c6W2suGacjQ">WIRED Video on the Nuclear Tests&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/tsOrRWzmmUU">Answering FAQs about the Nuclear Test Films&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtube.com/playlist?list=PLvGO_dWo8VfcmG166wKRy5z-GlJ_OQND5">Watch the recently-declassified original nuclear tests&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>About</title><link>https://www.justinnuwin.com/homepage/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/homepage/about/</guid><description>Hi there! My name is Justin and I am currently a masters student at Carnegie Mellon University (CMU) in the Electrical and Computer Engineering Program (ECE). My interests are focused in embedded systems, machine learning, and computer architecture. I am working on several proejcts relating to edge-ML: how we can apply computationally intensive algorithms on resource-constrained embedded and mobile platforms. I am really passionate about collaborating with others to build amazing things, so browse my projects and feel free to reach out!</description><content>&lt;p>&lt;strong>Hi there!&lt;/strong> My name is Justin and I am currently a masters student at Carnegie Mellon University (CMU) in the Electrical and Computer Engineering Program (ECE).
My interests are focused in embedded systems, machine learning, and computer architecture.
I am working on several proejcts relating to edge-ML: how we can apply computationally intensive algorithms on resource-constrained embedded and mobile platforms.
I am really passionate about collaborating with others to build amazing things, so browse my &lt;a href="https://www.justinnuwin.com/projects">projects&lt;/a> and feel free to &lt;a href="#contact">reach out&lt;/a>!&lt;/p>
&lt;p>Outside of software development and engineering, I love spending time outdoors, &lt;strong>vibing&lt;/strong> to 80&amp;rsquo;s jazz fusion music on my AT turntable, and cooking all kinds of international dishes!&lt;/p>
&lt;p>&lt;a href="https://github.com/justinnuwin">Github&lt;/a> • &lt;a href="https://www.linkedin.com/in/justinnuwin/">Linkedin&lt;/a> • &lt;a href="https://twitter.com/justinnuwin">Twitter&lt;/a> • &lt;a href="https://instagram.com/justinnuwin">Instagram&lt;/a>&lt;/p></content></item><item><title>Contact</title><link>https://www.justinnuwin.com/homepage/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/homepage/contact/</guid><description>Feel free to reach out to me via these platforms:
Github Linkedin Twitter</description><content>&lt;p>Feel free to reach out to me via these platforms:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/justinnuwin">Github&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.linkedin.com/in/justinnuwin/">Linkedin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://twitter.com/justinnuwin">Twitter&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Education</title><link>https://www.justinnuwin.com/homepage/education/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/homepage/education/</guid><description>Carnegie Mellon University M.S. Electrical and Computer Engineering • May 2020 - May 2021
Low Power code for IOT: Theory behind designing software (especially RTOS) to take advantage of &amp;ldquo;dark silicon&amp;rdquo; on embedded hardware to minimize energy use, thereby maximizing battery life. Speech Recognition and Understanding: Study how speech recognition systems model human speech using HMM+GMM, HMM+DNN, and end-to-end models. Implement simple HMM+GMM word recognizer and Listen, Attend, Spell (LAS) speech recognizer.</description><content>&lt;h2 id="carnegie-mellon-university">Carnegie Mellon University&lt;/h2>
&lt;p>&lt;em>M.S. Electrical and Computer Engineering&lt;/em> • May 2020 - May 2021&lt;/p>
&lt;ul>
&lt;li>Low Power code for IOT: Theory behind designing software (especially RTOS) to take advantage of &amp;ldquo;dark silicon&amp;rdquo; on embedded hardware to minimize energy use, thereby maximizing battery life.&lt;/li>
&lt;li>Speech Recognition and Understanding: Study how speech recognition systems model human speech using HMM+GMM, HMM+DNN, and end-to-end models. Implement simple HMM+GMM word recognizer and Listen, Attend, Spell (LAS) speech recognizer.&lt;/li>
&lt;li>Machine Learning for Signal Processing: Theory behind how machine learning can be applied to signal processing applications. Investigation of data driven signal processing approaches and parallels with machine learning techniques.&lt;/li>
&lt;li>Image and Video Processing: Theory and implementation of many popular image processing methods with particular emphasis on sparse approximations and linear inverse problems&lt;/li>
&lt;li>Machine Learning: Theory and implementation of various supervised and unsupervised machine learning algorithms.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="california-polytechnic-state-university">California Polytechnic State University&lt;/h2>
&lt;p>&lt;em>B.S. Computer Engineering&lt;/em> • Sep 2016 - Jun 2020&lt;/p>
&lt;ul>
&lt;li>Computer Architecture: Introductory computer architecture, ARM RISC instruction set, computing pipelines&lt;/li>
&lt;li>Implementation of Operating Systems: Implemented x86-64 operating system kernel from scratch&lt;/li>
&lt;li>Real Time Embedded Systems: Theory of SIMD and parallelized processing in embedded systems with implementation of parallelized processing on Digilent Zybo using ARM Linux and FPGA co-processor&lt;/li>
&lt;li>Computer Networks: Introductory computer networking covering OSI model, 802.3, TCP/IP, UDP, network switching/routing, and basic network security on Cisco appliances&lt;/li>
&lt;li>Capstone Project: Intelligent Sensing and Navigation System for NGCP (Northrop Grumman Collaboration Project) Unmanned Ground Vehicle&lt;/li>
&lt;li>Computer Vision: Introductory course on computer vision algorithms and their theory of operation building on simple operations like edge detection to full feature detection with modern algorithms like SIFT/SURF and neural networks&lt;/li>
&lt;li>Senior Project - To be published&lt;/li>
&lt;/ul></content></item><item><title>Skills</title><link>https://www.justinnuwin.com/homepage/skills/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.justinnuwin.com/homepage/skills/</guid><description>Python and packages such as Numpy, Pandas, PyTorch, Keras, and Sklearn C/C++ for desktop and embedded applications with experience with libraries such as OpenCV, Boost, gRPC, ProtoBuf Proficient with machine learning techniques and libraries. Experience working with computer vision and speech recognition Hardware design PCB layout in Allegro and Altium Experience with embedded systems, particularly: ultra-low power systems, interconnect protocols, network protocols, and RF design View all my projects and experiences by tag to find something more specific.</description><content>&lt;ul>
&lt;li>&lt;a href="https://www.justinnuwin.com/tags/python/">Python&lt;/a> and packages such as Numpy, Pandas, PyTorch, Keras, and Sklearn&lt;/li>
&lt;li>&lt;a href="https://www.justinnuwin.com/tags/c++">C/C++&lt;/a> for desktop and embedded applications with experience with libraries such as &lt;a href="https://www.justinnuwin.com/tags/opencv">OpenCV&lt;/a>, Boost, gRPC, ProtoBuf&lt;/li>
&lt;li>Proficient with &lt;a href="https://www.justinnuwin.com/tags/machine-learning">machine learning&lt;/a> techniques and libraries. Experience working with &lt;a href="https://www.justinnuwin.com/tags/computer-vision/">computer vision&lt;/a> and &lt;a href="https://www.justinnuwin.com/projects/asr-music-semester-project/">speech recognition&lt;/a>&lt;/li>
&lt;li>Hardware design &lt;a href="https://www.justinnuwin.com/tags/pcb-design/">PCB layout&lt;/a> in Allegro and Altium&lt;/li>
&lt;li>Experience with &lt;a href="https://www.justinnuwin.com/tags/embedded-systems/">embedded systems&lt;/a>, particularly: ultra-low power systems, interconnect protocols, network protocols, and RF design&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://www.justinnuwin.com/tags">View all my projects and experiences by tag to find something more specific.&lt;/a>&lt;/p></content></item></channel></rss>